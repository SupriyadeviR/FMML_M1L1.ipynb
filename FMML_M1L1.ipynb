{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SupriyadeviR/FMML_M1L1.ipynb/blob/main/FMML_M1L1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6I1yI79fbLD"
      },
      "source": [
        "# Extracting features from data\n",
        "\n",
        "FMML Module 1, Lab 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OimBnfcpvcNS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbfac351-252a-4d1b-8d1b-7e946f2a6dd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip -q install wikipedia nltk matplotlib plotly pandas\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "import wikipedia\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "from nltk.util import ngrams\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6hGhIGiy4GP"
      },
      "source": [
        "# Part 1: Features of text\n",
        "\n",
        "Computures can't understand text. They can only process numbers. So, the logical first step in any attempt to analyze text is to convert it into numbers. This process is called **feature extraction** or **vectorization**. In this lab, we will try some simple methods for feature extraction.\n",
        "\n",
        "First, let us download a 2 documents from Wikipedia in two different languages, English and French. We will then extract features from the text in these documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpUmCoEr2R3J"
      },
      "outputs": [],
      "source": [
        "topic1 = \"Giraffe\"\n",
        "topic2 = \"Elephant\"\n",
        "\n",
        "wikipedia.set_lang(\"en\")\n",
        "\n",
        "eng1 = wikipedia.page(topic1).content\n",
        "eng2 = wikipedia.page(topic2).content\n",
        "\n",
        "wikipedia.set_lang(\"fr\")\n",
        "\n",
        "fr1 = wikipedia.page(topic1).content\n",
        "fr2 = wikipedia.page(topic2).content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj7RlhMiO5kd"
      },
      "source": [
        "This is what the text looks like in English:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9D_8laDfupTt"
      },
      "outputs": [],
      "source": [
        "eng2[:500]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kdwJ30nupTt"
      },
      "source": [
        "This is what the text looks like in French:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GW0G-t912UXZ"
      },
      "outputs": [],
      "source": [
        "fr2[:500]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZkmNJ7XO9xX"
      },
      "source": [
        "We need to clean this up a bit. Let us remove all the special characters and keep only 26 letters and space. Note that this will remove accented characters in French also. We are also removing all the numbers and spaces. So this is not an ideal solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5yf5P9pPI4t"
      },
      "outputs": [],
      "source": [
        "def cleanup(text):\n",
        "    text = text.lower()  # make it lowercase\n",
        "    text = re.sub(\n",
        "        r\"[^a-z\\s]\", \"\", text\n",
        "    )  # only keep characters in a-z range and whitespaces\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrOjC32fRuTK"
      },
      "outputs": [],
      "source": [
        "eng1 = cleanup(eng1)\n",
        "eng2 = cleanup(eng2)\n",
        "fr1 = cleanup(fr1)\n",
        "fr2 = cleanup(fr2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIdqvL2G-LqL"
      },
      "outputs": [],
      "source": [
        "eng2[:500]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fr2[:500]"
      ],
      "metadata": {
        "id": "oAnXWFaFLaZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXFTWwd0rk63"
      },
      "source": [
        "Now let us calculate the frequency of the character n-grams. N-grams are groups of characters of size n. A unigram is a single character and a bigram is a group of two characters and so on.\n",
        "\n",
        "Let us count the frequency of each character in a text and plot it in a histogram."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3Lz3YUjN0L5"
      },
      "outputs": [],
      "source": [
        "def tuple2string(tup):\n",
        "    # convert a tuple of characters to a string\n",
        "    # ('t', 'h') - > 'th'\n",
        "    st = \"\"\n",
        "    for ii in tup:\n",
        "        st = st + ii\n",
        "    return st\n",
        "\n",
        "\n",
        "def key2string(keys):\n",
        "    # convert a tuple of tuples to a list of strings\n",
        "    # [('t', 'h'), ('h', 'e')] -> ['th', 'he']\n",
        "    # [('t')] - >['t']\n",
        "    return [tuple2string(i) for i in keys]\n",
        "\n",
        "\n",
        "def get_ngram_freq(ngram):\n",
        "    # get the frequency of ngrams\n",
        "    # sort the keys in alphabetic order\n",
        "    keys = key2string(ngram.keys())\n",
        "    values = list(ngram.values())\n",
        "\n",
        "    combined = zip(keys, values)\n",
        "    zipped_sorted = sorted(combined, key=lambda x: x[0])\n",
        "    keys, values = map(list, zip(*zipped_sorted))\n",
        "    return keys, values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHD62zbZcwAB"
      },
      "source": [
        "Let us compare the histograms of English pages and French pages. Can you spot a difference?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKcGRgH6b0KP"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
        "unigram_eng1 = Counter(ngrams(eng1, 1))\n",
        "keys, values = get_ngram_freq(unigram_eng1)\n",
        "axs[0].bar(keys, values)\n",
        "axs[0].set_title(\"English 1\")\n",
        "\n",
        "unigram_eng2 = Counter(ngrams(eng2, 1))\n",
        "keys, values = get_ngram_freq(unigram_eng2)\n",
        "axs[1].bar(keys, values)\n",
        "axs[1].set_title(\"English 2\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDM_UhCL2QLt"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
        "unigram_fr1 = Counter(ngrams(fr1, 1))\n",
        "keys, values = get_ngram_freq(unigram_fr1)\n",
        "axs[0].bar(keys, values)\n",
        "axs[0].set_title(\"French 1\")\n",
        "\n",
        "unigram_fr2 = Counter(ngrams(fr2, 1))\n",
        "keys, values = get_ngram_freq(unigram_fr2)\n",
        "axs[1].bar(keys, values)\n",
        "axs[1].set_title(\"French 2\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxgrdZLKdkAB"
      },
      "source": [
        "We can see that the unigrams for French and English are very similar. So this is not a good feature if we want to distinguish between English and French. Let us look at bigrams."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwPQbaTCupTu"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
        "bigram_eng1 = Counter(ngrams(eng1, 2))\n",
        "keys, values = get_ngram_freq(bigram_eng1)\n",
        "axs[0, 0].bar(keys, values)\n",
        "axs[0, 0].set_title(\"English 1\")\n",
        "\n",
        "bigram_eng2 = Counter(ngrams(eng2, 2))\n",
        "keys, values = get_ngram_freq(bigram_eng2)\n",
        "axs[0, 1].bar(keys, values)\n",
        "axs[0, 1].set_title(\"English 2\")\n",
        "\n",
        "bigram_fr1 = Counter(ngrams(fr1, 2))\n",
        "keys, values = get_ngram_freq(bigram_fr1)\n",
        "axs[1, 0].bar(keys, values)\n",
        "axs[1, 0].set_title(\"French 1\")\n",
        "\n",
        "bigram_fr2 = Counter(ngrams(fr2, 2))\n",
        "keys, values = get_ngram_freq(bigram_fr2)\n",
        "axs[1, 1].bar(keys, values)\n",
        "axs[1, 1].set_title(\"French 2\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-egsHMIg5Rp"
      },
      "source": [
        "Another way to visualize bigrams is to use a 2-dimensional graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EaPJgtaVxZM"
      },
      "outputs": [],
      "source": [
        "def get_2D_ngram_freq(ngram):\n",
        "    freq = np.zeros((26, 26))\n",
        "    for ii in range(26):\n",
        "        for jj in range(26):\n",
        "            freq[ii, jj] = ngram[(chr(ord(\"a\") + ii), chr(ord(\"a\") + jj))]\n",
        "    return freq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Hpu4zRVupTv"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
        "bigram_eng1_freq = get_2D_ngram_freq(bigram_eng1)\n",
        "fig.colorbar(axs[0, 0].imshow(bigram_eng1_freq, cmap=\"hot\"), ax=axs[0, 0])\n",
        "axs[0, 0].set_title(\"English 1\")\n",
        "\n",
        "bigram_eng2_freq = get_2D_ngram_freq(bigram_eng2)\n",
        "fig.colorbar(axs[0, 1].imshow(bigram_eng2_freq, cmap=\"hot\"), ax=axs[0, 1])\n",
        "axs[0, 1].set_title(\"English 2\")\n",
        "\n",
        "bigram_fr1_freq = get_2D_ngram_freq(bigram_fr1)\n",
        "fig.colorbar(axs[1, 0].imshow(bigram_fr1_freq, cmap=\"hot\"), ax=axs[1, 0])\n",
        "axs[1, 0].set_title(\"French 1\")\n",
        "\n",
        "bigram_fr2_freq = get_2D_ngram_freq(bigram_fr2)\n",
        "fig.colorbar(axs[1, 1].imshow(bigram_fr2_freq, cmap=\"hot\"), ax=axs[1, 1])\n",
        "axs[1, 1].set_title(\"French 2\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGOEHcyGokD0"
      },
      "source": [
        "Let us look at the top 10 ngrams for each text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dk2TkzTno8vb"
      },
      "outputs": [],
      "source": [
        "def ind2tup(ind):\n",
        "    ind = int(ind)\n",
        "    i = int(ind / 26)\n",
        "    j = int(ind % 26)\n",
        "    return (chr(ord(\"a\") + i), chr(ord(\"a\") + j))\n",
        "\n",
        "\n",
        "def ShowTopN(bifreq, n=10):\n",
        "    f = bifreq.flatten()\n",
        "    arg = np.argsort(-f)\n",
        "    for ii in range(n):\n",
        "        print(f\"{ind2tup(arg[ii])} : {f[arg[ii]]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HeWNh_q0QZ1"
      },
      "outputs": [],
      "source": [
        "print(\"\\nEnglish 1:\")\n",
        "ShowTopN(bigram_eng1_freq)\n",
        "\n",
        "print(\"\\nEnglish 2:\")\n",
        "ShowTopN(bigram_eng2_freq)\n",
        "\n",
        "print(\"\\nFrench 1:\")\n",
        "ShowTopN(bigram_fr1_freq)\n",
        "\n",
        "print(\"\\nFrench 2:\")\n",
        "ShowTopN(bigram_fr2_freq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kDovOP4l98z"
      },
      "source": [
        "We observe that the bigrams are similar across different topics but different across languages. Thus, the bigram frequency is a good feature for distinguishing languages, but not for distinguishing topics.\n",
        "\n",
        "Thus, we were able to convert a many-dimensional input (the text) to 26 dimesions (unigrams) or 26*26 dimensions (bigrams).\n",
        "\n",
        "\n",
        "A few ways to explore:\n",
        "1. Try with different languages.\n",
        "2. The topics we used are quite similar, wikipedia articles of 'elephant' and 'giraffe'. What happens if we use very different topics? What if we use text from another source than Wikipedia?\n",
        "3. How can we use and visualize trigrams and higher n-grams?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDHuf6v0upTv"
      },
      "source": [
        "> Exercise: Try to extract trigrams and visualize the top 10 trigrams for each text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZJfjIHk-oHV"
      },
      "source": [
        "# Part 2: Written numbers\n",
        "\n",
        "We've seen how to extract features from text. Now let us see how to extract features from images. We will use the MNIST dataset which contains images of handwritten numbers. Each image is represented in a 28*28 array. Let us see if we can extract some simple features from these images which can help us distinguish between the digits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNsLJSr6wGY0"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "# loading the dataset\n",
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVNr144WAUZO"
      },
      "source": [
        "Extract a subset of the data for our experiment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3MN8ddxAASZ"
      },
      "outputs": [],
      "source": [
        "no1 = train_X[train_y == 1, :, :]\n",
        "no0 = train_X[train_y == 0, :, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePXCs0qyCLpc"
      },
      "source": [
        "Let us visualize a few images here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQeyZSh-Arpc"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(2, 5, figsize=(15, 5))\n",
        "\n",
        "for ii in range(5):\n",
        "    axs[0, ii].imshow(no0[ii, :, :])\n",
        "\n",
        "for ii in range(5):\n",
        "    axs[1, ii].imshow(no1[ii, :, :])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1g-Tg7EKDz96"
      },
      "source": [
        "Now, let us start with a simple feature: the sum of all pixels. Let's see how good this feature is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8SztDk7CyZc"
      },
      "outputs": [],
      "source": [
        "sum1 = np.sum(no1 > 0, (1, 2))  # threshold before adding up\n",
        "sum0 = np.sum(no0 > 0, (1, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oW3XCOCE7Zv"
      },
      "source": [
        "Let us visualize how good this feature is: (X-axis is mean, y-axis is the digit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8PIe8o_DPpU"
      },
      "outputs": [],
      "source": [
        "plt.hist(sum1, alpha=0.7)\n",
        "plt.hist(sum0, alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_hToEepFtl2"
      },
      "source": [
        "We can already see that this feature separates the two classes quite well.\n",
        "\n",
        "Let us look at another, more complicated feature. We will count the number black pixels that are surrounded on four sides by non-black pixels, or \"hole pixels\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwEnlm6RFFej"
      },
      "outputs": [],
      "source": [
        "def cumArray(img):\n",
        "    img2 = img.copy()\n",
        "    for ii in range(1, img2.shape[1]):\n",
        "        img2[ii, :] = (\n",
        "            img2[ii, :] + img2[ii - 1, :]\n",
        "        )  # for every row, add up all the rows above it.\n",
        "    img2 = img2 > 0\n",
        "    return img2\n",
        "\n",
        "\n",
        "def getHolePixels(img):\n",
        "    im1 = cumArray(img)\n",
        "    im2 = np.rot90(\n",
        "        cumArray(np.rot90(img)), 3\n",
        "    )  # rotate and cumulate it again for differnt direction\n",
        "    im3 = np.rot90(cumArray(np.rot90(img, 2)), 2)\n",
        "    im4 = np.rot90(cumArray(np.rot90(img, 3)), 1)\n",
        "    hull = (\n",
        "        im1 & im2 & im3 & im4\n",
        "    )  # this will create a binary image with all the holes filled in.\n",
        "    # remove the original digit to leave behind the holes\n",
        "    hole = hull & ~(img > 0)\n",
        "    return hole"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iw3HjgnupUEI"
      },
      "source": [
        "Visualize a few. First row has the original zero number images and the second row has the hole pixels. Thrid row has original one number images and the last row has corresponding hole pixels which are non-existent, as expected."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_ids = [12, 236, 345, 435, 512]\n",
        "fig, axs = plt.subplots(4, 5, figsize=(15, 10))\n",
        "\n",
        "for ii, idx in enumerate(img_ids):\n",
        "    axs[0, ii].imshow(no0[idx, :, :])\n",
        "    axs[1, ii].imshow(getHolePixels(no0[idx, :, :]))\n",
        "    axs[2, ii].imshow(no1[idx, :, :])\n",
        "    axs[3, ii].imshow(getHolePixels(no1[idx, :, :]))"
      ],
      "metadata": {
        "id": "Ajzx6dkTwPst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vS-4erNXtxMi"
      },
      "source": [
        "Now let us plot the number of hole pixels and see how this feature behaves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dpm1dRgsety8"
      },
      "outputs": [],
      "source": [
        "hole1 = np.array([getHolePixels(i).sum() for i in no1])\n",
        "hole0 = np.array([getHolePixels(i).sum() for i in no0])\n",
        "\n",
        "plt.hist(hole1, alpha=0.7)\n",
        "plt.hist(hole0, alpha=0.7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UjCBHpJ31yq"
      },
      "source": [
        "This feature works even better to distinguish between one and zero.\n",
        "\n",
        "\n",
        "Now let us try the number of pixels in the 'hull' or the number with the holes filled in:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPtJ8eqolAOf"
      },
      "outputs": [],
      "source": [
        "def getHullPixels(img):\n",
        "    im1 = cumArray(img)\n",
        "    im2 = np.rot90(\n",
        "        cumArray(np.rot90(img)), 3\n",
        "    )  # rotate and cumulate it again for differnt direction\n",
        "    im3 = np.rot90(cumArray(np.rot90(img, 2)), 2)\n",
        "    im4 = np.rot90(cumArray(np.rot90(img, 3)), 1)\n",
        "    hull = (\n",
        "        im1 & im2 & im3 & im4\n",
        "    )  # this will create a binary image with all the holes filled in.\n",
        "    return hull"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_ids = [12, 236, 345, 435, 512]\n",
        "fig, axs = plt.subplots(4, 5, figsize=(15, 10))\n",
        "\n",
        "for ii, idx in enumerate(img_ids):\n",
        "    axs[0, ii].imshow(no0[idx, :, :])\n",
        "    axs[1, ii].imshow(getHullPixels(no0[idx, :, :]))\n",
        "    axs[2, ii].imshow(no1[idx, :, :])\n",
        "    axs[3, ii].imshow(getHullPixels(no1[idx, :, :]))"
      ],
      "metadata": {
        "id": "3fnBK7ImxtY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5rHal_HRWnE"
      },
      "source": [
        "Plotting the number of hull pixels versus the digit:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTLzYZLTRQ_p"
      },
      "outputs": [],
      "source": [
        "hull1 = np.array([getHullPixels(i).sum() for i in no1])\n",
        "hull0 = np.array([getHullPixels(i).sum() for i in no0])\n",
        "\n",
        "plt.hist(hull1, alpha=0.7)\n",
        "plt.hist(hull0, alpha=0.7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSzH26ElXNri"
      },
      "source": [
        "Let us try one more feature, where we look at the number of boundary pixels in each image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-2czBypXMwT"
      },
      "outputs": [],
      "source": [
        "def minus(a, b):\n",
        "    return a & ~b\n",
        "\n",
        "\n",
        "def getBoundaryPixels(img):\n",
        "    img = img.copy() > 0  # binarize the image\n",
        "    rshift = np.roll(img, 1, 1)\n",
        "    lshift = np.roll(img, -1, 1)\n",
        "    ushift = np.roll(img, -1, 0)\n",
        "    dshift = np.roll(img, 1, 0)\n",
        "    boundary = (\n",
        "        minus(img, rshift)\n",
        "        | minus(img, lshift)\n",
        "        | minus(img, ushift)\n",
        "        | minus(img, dshift)\n",
        "    )\n",
        "    return boundary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_ids = [12, 236, 345, 435, 512]\n",
        "fig, axs = plt.subplots(4, 5, figsize=(15, 10))\n",
        "\n",
        "for ii, idx in enumerate(img_ids):\n",
        "    axs[0, ii].imshow(no0[idx, :, :])\n",
        "    axs[1, ii].imshow(getBoundaryPixels(no0[idx, :, :]))\n",
        "    axs[2, ii].imshow(no1[idx, :, :])\n",
        "    axs[3, ii].imshow(getBoundaryPixels(no1[idx, :, :]))"
      ],
      "metadata": {
        "id": "kUOHZViDx5-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSsxsbCNXcNh"
      },
      "outputs": [],
      "source": [
        "bound1 = np.array([getBoundaryPixels(i).sum() for i in no1])\n",
        "bound0 = np.array([getBoundaryPixels(i).sum() for i in no0])\n",
        "\n",
        "plt.hist(bound1, alpha=0.7)\n",
        "plt.hist(bound0, alpha=0.7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuP04Ao_R0Yz"
      },
      "source": [
        "What will happen if we plot two features together?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kl7xWg-WRkAy"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 3, figsize=(20, 5))\n",
        "\n",
        "axs[0].scatter(sum0, hull0, alpha=0.1)\n",
        "axs[0].scatter(sum1, hull1, alpha=0.1)\n",
        "axs[0].set_xlabel(\"Sum\")\n",
        "axs[0].set_ylabel(\"Hull\")\n",
        "axs[0].legend([\"0\", \"1\"])\n",
        "\n",
        "axs[1].scatter(sum0, hole0, alpha=0.1)\n",
        "axs[1].scatter(sum1, hole1, alpha=0.1)\n",
        "axs[1].set_xlabel(\"Sum\")\n",
        "axs[1].set_ylabel(\"Hole\")\n",
        "axs[1].legend([\"0\", \"1\"])\n",
        "\n",
        "axs[2].scatter(bound0, hole0, alpha=0.1)\n",
        "axs[2].scatter(bound1, hole1, alpha=0.1)\n",
        "axs[2].set_xlabel(\"Boundary\")\n",
        "axs[2].set_ylabel(\"Hole\")\n",
        "axs[2].legend([\"0\", \"1\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JYLmKNFSIT-"
      },
      "source": [
        "Now let us try plotting 3 features together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOKEHIXFaWp_"
      },
      "outputs": [],
      "source": [
        "cl1 = [\"class 1\"] * len(sum1)\n",
        "cl0 = [\"class 0\"] * len(sum0)\n",
        "df = pd.DataFrame(\n",
        "    list(\n",
        "        zip(\n",
        "            np.concatenate((hole0, hole0)),\n",
        "            np.concatenate((sum1, sum0)),\n",
        "            np.concatenate((bound1, bound0)),\n",
        "            np.concatenate((cl1, cl0)),\n",
        "        )\n",
        "    ),\n",
        "    columns=[\"Hole\", \"Sum\", \"Boundary\", \"Class\"],\n",
        ")\n",
        "df.head()\n",
        "fig = px.scatter_3d(df, x=\"Hole\", y=\"Sum\", z=\"Boundary\",\n",
        "                    color=\"Class\", opacity=0.1)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paDGHlFSd5Fu"
      },
      "source": [
        "Feel free to explore the above graph with your mouse.\n",
        "\n",
        "\n",
        "We extracted four features from a 28*28 dimensional image.\n",
        "\n",
        "\n",
        "Some questions to explore:\n",
        "1. Which is the best combination of features?\n",
        "vTo help you determine the best combination of features, I need more context. Could you clarify what features you're referring to? For example, are you asking about product features, software options, or characteristics in a specific context? The more details you provide, the better I can assist you.\n",
        "2. How would you test or visualize four or more features?\n",
        "Testing or visualizing four or more features depends on the type of data you're working with and the goals of your analysis. Here are some common methods:\n",
        "\n",
        "### 1. *Scatter Plot Matrix (Pair Plot)*\n",
        "   - *What it is:* A grid of scatter plots showing relationships between each pair of features.\n",
        "   - *When to use:* When you want to visualize pairwise relationships among features.\n",
        "   - *Tools:* Seaborn’s pairplot() or Pandas scatter_matrix().\n",
        "\n",
        "### 2. *Heatmap*\n",
        "   - *What it is:* A visual representation of data where individual values are represented by colors.\n",
        "   - *When to use:* To show the correlation between features.\n",
        "   - *Tools:* Seaborn’s heatmap().\n",
        "\n",
        "### 3. *Parallel Coordinates Plot*\n",
        "   - *What it is:* A plot that represents each feature as a vertical line, with each data point as a line connecting them.\n",
        "   - *When to use:* To visualize patterns and trends across multiple features.\n",
        "   - *Tools:* Matplotlib’s parallel_coordinates() or Plotly.\n",
        "\n",
        "### 4. *Principal Component Analysis (PCA)*\n",
        "   - *What it is:* A dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional space.\n",
        "   - *When to use:* To visualize how the features relate in a reduced dimension (2D or 3D).\n",
        "   - *Tools:* Scikit-learn’s PCA() or Seaborn's pairplot() after applying PCA.\n",
        "\n",
        "### 5. *Radial Plot (Spider Plot)*\n",
        "   - *What it is:* A plot that represents each feature as a spoke on a wheel, with data points plotted as a polygon.\n",
        "   - *When to use:* To compare multiple features at once.\n",
        "   - *Tools:* Matplotlib’s radar_chart().\n",
        "\n",
        "### 6. *3D Scatter Plot*\n",
        "   - *What it is:* A three-dimensional scatter plot where each axis represents one feature, and color or size can represent additional features.\n",
        "   - *When to use:* When you have three features to visualize simultaneously, and the fourth can be represented by color, size, or shape.\n",
        "   - *Tools:* Matplotlib, Plotly.\n",
        "\n",
        "### 7. *t-SNE or UMAP*\n",
        "   - *What it is:* Non-linear dimensionality reduction techniques that preserve local structures.\n",
        "   - *When to use:* When you want to visualize clusters or patterns in high-dimensional data.\n",
        "   - *Tools:* Scikit-learn, UMAP-learn.\n",
        "\n",
        "### 8. *Feature Importance Plot*\n",
        "   - *What it is:* A bar plot that shows the relative importance of each feature for a specific model.\n",
        "   - *When to use:* To determine which features contribute most to the model's predictions.\n",
        "   - *Tools:* Scikit-learn’s plot_importance() or XGBoost’s plot_importance().\n",
        "\n",
        "### 9. *Contour Plots*\n",
        "   - *What it is:* A plot that shows the joint distribution of two features with additional dimensions encoded in color or contour lines.\n",
        "   - *When to use:* To visualize how the distribution of one feature changes with respect to another while considering additional features.\n",
        "   - *Tools:* Matplotlib’s contour() or contourf().\n",
        "\n",
        "### 10. *Box Plot*\n",
        "   - *What it is:* A graphical representation of data distribution through quartiles.\n",
        "   - *When to use:* To compare distributions of multiple features.\n",
        "   - *Tools:* Seaborn’s boxplot().\n",
        "\n",
        "### Practical Example:\n",
        "If you have four features (say, A, B, C, and D), you could:\n",
        "1. Use a scatter plot matrix to see pairwise relationships.\n",
        "2. Apply PCA to reduce dimensionality and visualize in 2D.\n",
        "3. Use a heatmap to examine correlations.\n",
        "\n",
        "Would you like more specific examples or a walkthrough of one of these methods?\n",
        "\n",
        "3. Can you come up with your own features?\n",
        "Certainly! Creating features, also known as \"feature engineering,\" is a critical part of data analysis and machine learning. Here are a few examples of how you might create new features based on existing data in different contexts:\n",
        "\n",
        "### 1. *E-Commerce:*\n",
        "   - *Existing Data:* Customer purchase history, product prices, and timestamps.\n",
        "   - *New Features:*\n",
        "     - *Average Order Value (AOV):* Average amount spent per order by each customer.\n",
        "     - *Days Since Last Purchase:* Time since the customer last made a purchase.\n",
        "     - *Purchase Frequency:* Number of purchases per unit of time (e.g., per month).\n",
        "     - *Discount Rate:* Difference between original price and purchase price, indicating sensitivity to discounts.\n",
        "\n",
        "### 2. *Real Estate:*\n",
        "   - *Existing Data:* Property size, location, number of rooms, and age of the property.\n",
        "   - *New Features:*\n",
        "     - *Price Per Square Foot:* Total price divided by the area of the property.\n",
        "     - *Room-to-Space Ratio:* Number of rooms divided by the square footage.\n",
        "     - *Age of Property in Years:* Current year minus the year the property was built.\n",
        "     - *Proximity to Amenities:* Distance to nearest school, park, or shopping center.\n",
        "\n",
        "### 3. *Health & Fitness:*\n",
        "   - *Existing Data:* Daily step count, calorie intake, hours of sleep, and workout type.\n",
        "   - *New Features:*\n",
        "     - *Active vs. Sedentary Ratio:* Ratio of active minutes to sedentary minutes per day.\n",
        "     - *Calories Burned per Step:* Calculated using step count and personal metrics like weight.\n",
        "     - *Sleep Efficiency:* Ratio of actual sleep time to time spent in bed.\n",
        "     - *Workout Intensity Score:* Derived from the type of workout, duration, and heart rate.\n",
        "\n",
        "### 4. *Financial Analysis:*\n",
        "   - *Existing Data:* Stock prices, trading volume, and economic indicators.\n",
        "   - *New Features:*\n",
        "     - *Moving Average:* Average stock price over a specific period (e.g., 50-day moving average).\n",
        "     - *Price Momentum:* Rate of change in stock price over time.\n",
        "     - *Volatility Index:* Measure of the stock's price fluctuations over a period.\n",
        "     - *Price-to-Earnings Growth (PEG) Ratio:* Price-to-earnings ratio divided by the growth rate of earnings.\n",
        "\n",
        "### 5. *Social Media Analysis:*\n",
        "   - *Existing Data:* Number of likes, comments, shares, and post frequency.\n",
        "   - *New Features:*\n",
        "     - *Engagement Rate:* Total engagement (likes, comments, shares) divided by the number of followers.\n",
        "     - *Post Impact Score:* Weighted score based on engagement and the reach of a post.\n",
        "     - *Time Between Posts:* Average time interval between posts.\n",
        "     - *Sentiment Score:* Derived from natural language processing (NLP) analysis of comments.\n",
        "\n",
        "### 6. *Retail:*\n",
        "   - *Existing Data:* Product sales data, customer demographics, and store locations.\n",
        "   - *New Features:*\n",
        "     - *Sales Growth Rate:* Rate at which sales are increasing or decreasing over time.\n",
        "     - *Customer Lifetime Value (CLV):* Predicted revenue a customer will generate over their relationship with the business.\n",
        "     - *Seasonal Index:* Sales performance adjusted for seasonal patterns.\n",
        "     - *Customer Segmentation:* Grouping customers based on purchasing behavior, demographics, or engagement.\n",
        "\n",
        "### 7. *Marketing:*\n",
        "   - *Existing Data:* Campaign budget, conversion rate, and ad impressions.\n",
        "   - *New Features:*\n",
        "     - *Return on Ad Spend (ROAS):* Revenue generated from ad campaigns divided by the ad spend.\n",
        "     - *Cost per Acquisition (CPA):* Total cost divided by the number of new customers acquired.\n",
        "     - *Click-Through Rate (CTR):* Number of clicks divided by the number of impressions.\n",
        "     - *Customer Journey Duration:* Time taken from the first interaction to purchase.\n",
        "\n",
        "Creating new features involves understanding the domain, identifying relationships or patterns in the data, and then crafting features that can help in making predictions or understanding the data better.\n",
        "\n",
        "If you have a specific domain or dataset in mind, I can suggest more targeted features.\n",
        "4. Will these features work for different classes other than 0 and 1?\n",
        "Yes, the features mentioned above can work for different classes beyond binary classes like 0 and 1. These features are generally applicable to multi-class classification, regression tasks, or other forms of analysis. Here’s how:\n",
        "\n",
        "### 1. *Multi-Class Classification:*\n",
        "   - *Example:* Predicting the category of a product (e.g., electronics, clothing, food).\n",
        "   - *Feature Application:*\n",
        "     - *Engagement Rate:* Useful across multiple product categories to see which types of products engage customers more.\n",
        "     - *Price Per Square Foot (Real Estate):* Can differentiate between luxury, mid-range, and budget properties.\n",
        "     - *Workout Intensity Score (Health & Fitness):* Can distinguish between different levels of workout intensity, which may correlate with various fitness goals or outcomes.\n",
        "\n",
        "### 2. *Regression Tasks:*\n",
        "   - *Example:* Predicting continuous outcomes, like house prices or stock returns.\n",
        "   - *Feature Application:*\n",
        "     - *Moving Average (Financial):* A good predictor of stock price trends, which is not limited to binary outcomes.\n",
        "     - *Customer Lifetime Value (Retail):* A continuous feature predicting revenue generated by a customer.\n",
        "     - *Sales Growth Rate (Retail):* Useful in predicting future sales, which is a continuous outcome.\n",
        "\n",
        "### 3. *Multi-Class or Ordinal Classification:*\n",
        "   - *Example:* Predicting academic grades (A, B, C, etc.) or customer satisfaction levels (Very Satisfied, Satisfied, Neutral, Unsatisfied, Very Unsatisfied).\n",
        "   - *Feature Application:*\n",
        "     - *Sentiment Score (Social Media):* Can distinguish between various levels of customer satisfaction.\n",
        "     - *Proximity to Amenities (Real Estate):* Can affect property values across different classes (e.g., residential, commercial, industrial).\n",
        "     - *Cost per Acquisition (Marketing):* Can be used to analyze the effectiveness of different marketing channels, each being a different class.\n",
        "\n",
        "### 4. *Unsupervised Learning:*\n",
        "   - *Example:* Clustering customers into different segments.\n",
        "   - *Feature Application:*\n",
        "     - *Customer Segmentation (Retail):* Can be used to group customers without predefined classes, and the resulting clusters can be more than two.\n",
        "     - *Purchase Frequency (E-Commerce):* Useful for clustering customers based on how often they shop.\n",
        "     - *Time Between Posts (Social Media):* Can help in clustering users by their activity levels.\n",
        "\n",
        "### 5. *Anomaly Detection:*\n",
        "   - *Example:* Detecting fraudulent transactions in a multi-class setting.\n",
        "   - *Feature Application:*\n",
        "     - *Volatility Index (Financial):* Can indicate unusual fluctuations in stock prices, which might be labeled differently in a multi-class context (e.g., minor fluctuation, significant fluctuation, etc.).\n",
        "     - *Engagement Rate (Social Media):* Could flag unusual user activity that deviates from typical engagement patterns.\n",
        "\n",
        "### *Why These Features Work Beyond 0 and 1:*\n",
        "- *Scalability:* The features are continuous or categorical, making them adaptable to more than just binary classification.\n",
        "- *Generalizability:* These features capture inherent properties of the data (e.g., trends, ratios, frequencies) that apply to various classes, not just binary ones.\n",
        "- *Flexibility:* They can be used with different types of machine learning models (e.g., decision trees, neural networks, clustering algorithms) that support multi-class and continuous output.\n",
        "\n",
        "### Adapting Features for Multi-Class Use:\n",
        "- *Normalization/Scaling:* Ensure features are scaled appropriately, especially when dealing with multiple classes with different ranges.\n",
        "- *Feature Selection:* Some features might be more relevant for certain classes, so using techniques like feature importance ranking can help refine them for multi-class problems.\n",
        "- *Transformation:* Sometimes, you may need to create additional features or transform existing ones to capture relationships unique to multi-class settings.\n",
        "\n",
        "In summary, the features you engineer should generally be applicable to different types of classes, not just binary ones. They should be tested and validated to ensure they provide meaningful insights or predictive power in the specific context you’re working with.\n",
        "\n",
        "5. What will happen if we take more that two classes at a time?\n",
        "When dealing with more than two classes in a machine learning or data analysis context, the process and outcomes differ from binary classification in several key ways. Here’s what happens:\n",
        "\n",
        "### 1. *Complexity Increases:*\n",
        "   - *Model Complexity:* The model must distinguish among more classes, which increases the complexity. For example, in a three-class problem, the model must learn boundaries between each pair of classes.\n",
        "   - *Decision Boundaries:* Instead of a single decision boundary, the model must create multiple boundaries to separate the classes, which can be more intricate and harder to visualize.\n",
        "\n",
        "### 2. *Evaluation Metrics:*\n",
        "   - *Confusion Matrix:* A multi-class confusion matrix is larger and more complex, with dimensions equal to the number of classes (e.g., 3x3 for three classes). It shows how well the model is performing across all classes.\n",
        "   - *Accuracy:* The overall accuracy metric can become less informative as it doesn't show how the model performs on individual classes. For this reason, class-specific metrics like precision, recall, and F1-score are often reported for each class.\n",
        "   - *Precision and Recall:* For multi-class problems, these metrics are calculated per class, and their average (micro, macro, or weighted) is often reported to get an overall sense of model performance.\n",
        "\n",
        "### 3. *Model Strategies for Multi-Class Classification:*\n",
        "   - *One-vs-Rest (OvR):* The problem is split into multiple binary classification problems. For example, in a 3-class problem (A, B, C), the model is trained on \"A vs. not A\", \"B vs. not B\", and \"C vs. not C\". The final prediction is based on the highest confidence score among these models.\n",
        "   - *One-vs-One (OvO):* The model is trained on every possible pair of classes (e.g., A vs. B, A vs. C, B vs. C). For \\( k \\) classes, \\( \\frac{k(k-1)}{2} \\) classifiers are built. The final prediction is made by combining the results of all the classifiers (often using a voting system).\n",
        "   - *Multinomial/Softmax:* Some models, like multinomial logistic regression or neural networks, inherently support multi-class classification by directly optimizing for all classes simultaneously. Softmax is commonly used in neural networks to output probabilities across all classes.\n",
        "\n",
        "### 4. *Data Requirements:*\n",
        "   - *More Data Needed:* Multi-class models generally require more data to learn effectively. Each class needs sufficient representation to ensure the model generalizes well.\n",
        "   - *Class Imbalance:* Class imbalance can become a significant issue in multi-class problems. If one class dominates, the model might perform poorly on underrepresented classes. Techniques like resampling, weighting classes, or using specialized loss functions (e.g., focal loss) are often used to address this.\n",
        "\n",
        "### 5. *Interpretability:*\n",
        "   - *Interpretation Challenges:* Understanding and interpreting model behavior becomes more challenging with more classes, especially if the model is non-linear (e.g., a deep neural network). Feature importance, partial dependence plots, and other tools can help, but the complexity increases.\n",
        "   - *Decision Trees and Rules:* For some models like decision trees, the rules and paths become more complex, making it harder to interpret the decision-making process.\n",
        "\n",
        "### 6. *Visualization:*\n",
        "   - *Visualizing Data:* Visualizing multi-class data is more challenging, especially when the number of features is high. Techniques like t-SNE, UMAP, and PCA can help reduce dimensionality for visualization, but interpreting these plots is still more complex than in binary cases.\n",
        "   - *Decision Boundaries:* Visualizing decision boundaries in multi-class problems requires advanced techniques, especially if there are more than three classes. Tools that plot decision regions can become cluttered, making it harder to see clear distinctions.\n",
        "\n",
        "### 7. *Performance and Training Time:*\n",
        "   - *Longer Training Time:* Training time generally increases with the number of classes because the model needs to process more information and learn more complex patterns.\n",
        "   - *Higher Resource Usage:* More classes often lead to increased computational resources (memory, processing power), especially with large datasets.\n",
        "\n",
        "### 8. *Probabilistic Outputs:*\n",
        "   - *Softmax Probabilities:* In models like neural networks, the output layer often uses a softmax function to provide a probability distribution across all classes, rather than a single probability as in binary classification.\n",
        "   - *Calibration:* The probabilities predicted by the model need to be well-calibrated, especially in cases where misclassification costs vary among classes.\n",
        "\n",
        "### Example Scenarios:\n",
        "- *Medical Diagnosis:* If predicting the type of disease (e.g., flu, cold, COVID-19), the model needs to distinguish among all possible diseases rather than just \"disease\" vs. \"no disease.\"\n",
        "- *Image Classification:* In a dataset like CIFAR-10, with 10 classes of objects, the model needs to correctly classify each image into one of the 10 categories.\n",
        "- *Sentiment Analysis:* If classifying reviews into \"positive,\" \"neutral,\" and \"negative,\" the model must manage the nuance between different sentiments.\n",
        "\n",
        "In summary, handling more than two classes introduces additional complexity and challenges, but it is entirely feasible with the right strategies and understanding of the domain. The key is to choose the appropriate model, preprocessing, and evaluation techniques to ensure robust performance across all classes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Exercise: Remember we took a subset of only the first two numbers in MNIST? Include 5 or more numbers now and try to visualise which feature works best when multiple numbers are involved. Brownie points if you use all numbers :)"
      ],
      "metadata": {
        "id": "vHE2yqmpzOXs"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}